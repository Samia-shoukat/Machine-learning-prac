# for non-linear prediction
# for both regression and classification


# manhattan distance= L1 = |x2-x1| +|y2-y1|
# Euclidean distance  = L2 = underroot((x2-x1)sqr-(y2-y1)sqr)


import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from mlxtend.plotting import plot_decision_regions


dataset = pd.read_csv(r"E:\Machine Learning\social_network_ads.csv")


dataset.head(4)


sns.scatterplot(x="Age", y="EstimatedSalary", data=dataset, hue="Purchased")
plt.show()





x = dataset.iloc[:,:-1]
y = dataset["Purchased"]


from sklearn.preprocessing import StandardScaler



sc = StandardScaler()
sc.fit(x)
s = pd.DataFrame(sc.transform(x),columns=x.columns)
                 


s


from sklearn.model_selection import train_test_split


x_train, x_test, y_train, y_test =train_test_split(x,y,test_size=0.2, random_state=42)



from sklearn.neighbors import KNeighborsClassifier


knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(x_train,y_train)


knn.score(x_test,y_test)*100


knn.score(x_train,y_train)*100





for i in range(1,30):
    knn1 = KNeighborsClassifier(n_neighbors=i)
    knn1.fit(x_train,y_train)
    print(i,knn1.score(x_train,y_train)*100,knn1.score(x_test,y_test)*100)


knn.predict([[49,36000]])


x


x,y


plot_decision_regions(x.to_numpy(),y.to_numpy(),clf=knn)
plt.show()



